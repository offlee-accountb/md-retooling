#!/usr/bin/env python3
"""
HWPX í•„ë“œ ë¶„ì„ê¸° (Field Extractor)
===================================

HWPX ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬:
1. ì…ë ¥ì´ í•„ìš”í•œ í•„ë“œ ëª©ë¡ ì¶”ì¶œ
2. ë°ì´í„° ìˆ˜ì§‘ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
3. ë¹ˆ ì…ë ¥ í…œí”Œë¦¿ YAML ìƒì„±

ì‚¬ìš©ë²•:
    python field_extractor.py input.hwpx -o output_prefix
    
ì¶œë ¥:
    - {output_prefix}_fields.yaml    : ë¹ˆ ì…ë ¥ í…œí”Œë¦¿
    - {output_prefix}_prompt.md      : ë°ì´í„° ìˆ˜ì§‘ìš© í”„ë¡¬í”„íŠ¸
"""

import argparse
import zipfile
import re
import xml.etree.ElementTree as ET
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import List, Dict, Set, Tuple

# XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤
HP_NS = '{http://www.hancom.co.kr/hwpml/2011/paragraph}'

# ì•Œë ¤ì§„ ë¼ë²¨ íŒ¨í„´ (ì •ê·œí™”ìš©)
KNOWN_LABELS = {
    # ê¸°ì—… ì •ë³´
    "ê¸°ì—…ëª…": ["ê¸°ì—…ì²´ëª…", "íšŒì‚¬ëª…", "ìƒí˜¸", "ë²•ì¸ëª…", "ì—…ì²´ëª…"],
    "ëŒ€í‘œì": ["ëŒ€í‘œì´ì‚¬", "ëŒ€í‘œìëª…", "ëŒ€í‘œ", "ëŒ€í‘œìë˜ëŠ”ëŒ€í‘œì´ì‚¬"],
    "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸": ["ì‚¬ì—…ìë²ˆí˜¸", "ì‚¬ì—…ìë“±ë¡", "ë“±ë¡ë²ˆí˜¸", "ì‚¬ì—…ìë²ˆí˜¸ë˜ëŠ”ë²•ì¸ë²ˆí˜¸"],
    "ì„¤ë¦½ì¼ì": ["ì„¤ë¦½ì¼", "ì°½ì—…ì¼", "ì°½ì—…ì¼ì", "ê°œì—…ì¼"],
    "ì£¼ì†Œ": ["ì†Œì¬ì§€", "ì‚¬ì—…ì¥ì£¼ì†Œ", "ë³¸ì‚¬ì£¼ì†Œ", "íšŒì‚¬ì£¼ì†Œ"],
    
    # ë‹´ë‹¹ì ì •ë³´
    "ì„±ëª…": ["ì´ë¦„", "ë‹´ë‹¹ìëª…", "ë‹´ë‹¹ì", "ì‘ì„±ì"],
    "ë¶€ì„œëª…": ["ë¶€ì„œ", "ì†Œì†ë¶€ì„œ", "ì†Œì†"],
    "ì§ìœ„": ["ì§ì±…", "ì§ê¸‰"],
    "íœ´ëŒ€í°": ["íœ´ëŒ€ì „í™”", "í•¸ë“œí°", "ëª¨ë°”ì¼", "HP"],
    "ì „í™”": ["ì „í™”ë²ˆí˜¸", "ìœ ì„ ì „í™”", "TEL"],
    "ì´ë©”ì¼": ["E-mail", "email", "ë©”ì¼", "ì´ë©”ì¼ì£¼ì†Œ"],
    "ì—°ë½ì²˜": ["ì „í™”", "ì „í™”ë²ˆí˜¸"],
    
    # ì‹ ì²­ ì •ë³´
    "ì‹ ì²­ê¸°ì—…ëª…": ["ì‹ ì²­ê¸°ì—…", "ì‹ ì²­ì—…ì²´ëª…"],
    "ì‹ ì²­ì": ["ì‹ ì²­ì¸", "ì‹ ì²­ìëª…"],
    "ì‹ ì²­ì¼": ["ì‹ ì²­ì¼ì", "ì ‘ìˆ˜ì¼"],
    
    # ìˆ˜ì‹ ì
    "ê·€ì¤‘": ["ê·€í•˜", "ì•"],
}

# ì…ë ¥ ì˜ˆì‹œ
FIELD_EXAMPLES = {
    "ê¸°ì—…ëª…": "(ì£¼)ìŠ¤ë§ˆíŠ¸í…Œí¬, ABC ì£¼ì‹íšŒì‚¬",
    "ëŒ€í‘œì": "í™ê¸¸ë™",
    "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸": "123-45-67890 (10ìë¦¬)",
    "ì„¤ë¦½ì¼ì": "2018-03-15 ë˜ëŠ” 2018ë…„ 3ì›” 15ì¼",
    "ì£¼ì†Œ": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
    "ë¶€ì„œëª…": "ê¸°ìˆ ê°œë°œíŒ€, ê²½ì˜ì§€ì›íŒ€",
    "ì§ìœ„": "íŒ€ì¥, ê³¼ì¥, ëŒ€ë¦¬",
    "ì„±ëª…": "ê¹€ë‹´ë‹¹",
    "íœ´ëŒ€í°": "010-1234-5678",
    "ì „í™”": "02-555-1234",
    "ì´ë©”ì¼": "contact@company.co.kr",
    "ì—°ë½ì²˜": "02-555-1234",
    "ì‹ ì²­ê¸°ì—…ëª…": "ê¸°ì—…ëª…ê³¼ ë™ì¼",
    "ì‹ ì²­ì": "ëŒ€í‘œìì™€ ë™ì¼",
    "ì‹ ì²­ì¼": "ìë™ì…ë ¥(ì˜¤ëŠ˜) ë˜ëŠ” 2025-11-30",
    "ê·€ì¤‘": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ì¥ê´€, OOê¸°ê´€ì¥",
}


@dataclass
class FieldInfo:
    """ì¶”ì¶œëœ í•„ë“œ ì •ë³´"""
    label: str                  # ì›ë³¸ ë¼ë²¨
    normalized: str             # ì •ê·œí™”ëœ ë¼ë²¨
    section: str                # ë°œê²¬ëœ ì„¹ì…˜
    page_hint: str              # í˜ì´ì§€ íŒíŠ¸
    context: str                # ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸
    pattern_type: str           # íŒ¨í„´ ìœ í˜• (table/paragraph/signature)
    example: str = ""           # ì…ë ¥ ì˜ˆì‹œ


def normalize_label(text: str) -> str:
    """ë¼ë²¨ ì •ê·œí™”: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    if not text:
        return ""
    # ê³µë°± ì œê±° (í•œê¸€ ì‚¬ì´ ê³µë°± í¬í•¨)
    result = re.sub(r'\s+', '', text)
    # ì½œë¡  ì œê±°
    result = result.replace(':', '').replace('ï¼š', '')
    return result


def find_canonical_label(label: str) -> str:
    """ì •ê·œí™”ëœ ë¼ë²¨ì„ ëŒ€í‘œ ë¼ë²¨ë¡œ ë³€í™˜"""
    norm = normalize_label(label)
    
    # ì§ì ‘ ë§¤ì¹­
    for canonical, variants in KNOWN_LABELS.items():
        if norm == normalize_label(canonical):
            return canonical
        for v in variants:
            if norm == normalize_label(v):
                return canonical
    
    # ë¶€ë¶„ ë§¤ì¹­ (í¬í•¨ ê´€ê³„)
    for canonical, variants in KNOWN_LABELS.items():
        if normalize_label(canonical) in norm or norm in normalize_label(canonical):
            return canonical
        for v in variants:
            if normalize_label(v) in norm or norm in normalize_label(v):
                return canonical
    
    return label  # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜


def is_label_candidate(text: str) -> bool:
    """ë¼ë²¨ í›„ë³´ì¸ì§€ íŒë‹¨"""
    if not text or len(text) > 30:
        return False
    
    text = text.strip()
    
    # ë¹ˆ ê°’, ìˆ«ìë§Œ, íŠ¹ìˆ˜ë¬¸ìë§Œ ì œì™¸
    if not text or text.isdigit():
        return False
    
    # ì½œë¡ ìœ¼ë¡œ ëë‚˜ë©´ ë¼ë²¨ ê°€ëŠ¥ì„± ë†’ìŒ
    if text.endswith(':') or text.endswith('ï¼š'):
        return True
    
    # ì•Œë ¤ì§„ ë¼ë²¨ íŒ¨í„´ ì²´í¬
    norm = normalize_label(text)
    for canonical, variants in KNOWN_LABELS.items():
        if norm == normalize_label(canonical):
            return True
        for v in variants:
            if norm == normalize_label(v):
                return True
    
    return False


def extract_table_fields(root, section_name: str) -> List[FieldInfo]:
    """í…Œì´ë¸”ì—ì„œ í•„ë“œ ì¶”ì¶œ"""
    fields = []
    seen_labels = set()
    
    for tbl_idx, tbl in enumerate(root.iter(f'{HP_NS}tbl')):
        for tr in tbl.iter(f'{HP_NS}tr'):
            cells = list(tr.findall(f'{HP_NS}tc'))
            
            for cell_idx, tc in enumerate(cells):
                # ì…€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                texts = [t.text for t in tc.iter(f'{HP_NS}t') if t.text]
                cell_text = ''.join(texts).strip()
                
                if not is_label_candidate(cell_text):
                    continue
                
                # ë‹¤ìŒ ì…€ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸ (ì…ë ¥ í•„ë“œ ê°€ëŠ¥ì„±)
                has_empty_next = False
                if cell_idx + 1 < len(cells):
                    next_texts = [t.text for t in cells[cell_idx + 1].iter(f'{HP_NS}t') if t.text]
                    next_text = ''.join(next_texts).strip()
                    if not next_text or next_text in ['', ' ', 'ã€€']:
                        has_empty_next = True
                
                # PREPEND íŒ¨í„´ ì²´í¬ (ë°‘ì¤„ + ë¼ë²¨)
                runs = list(tc.iter(f'{HP_NS}run'))
                is_prepend = False
                if len(runs) >= 2:
                    for i, run in enumerate(runs):
                        t = run.find(f'{HP_NS}t')
                        if t is not None and t.text:
                            if normalize_label(cell_text) in normalize_label(t.text):
                                if i > 0:
                                    prev_t = runs[i-1].find(f'{HP_NS}t')
                                    if prev_t is not None and prev_t.text:
                                        if prev_t.text.strip() == '' and len(prev_t.text) >= 5:
                                            is_prepend = True
                
                if has_empty_next or is_prepend:
                    canonical = find_canonical_label(cell_text)
                    
                    if canonical not in seen_labels:
                        seen_labels.add(canonical)
                        fields.append(FieldInfo(
                            label=cell_text,
                            normalized=canonical,
                            section=section_name,
                            page_hint=f"í…Œì´ë¸” {tbl_idx + 1}",
                            context=f"í…Œì´ë¸” ë‚´ ë¼ë²¨",
                            pattern_type="prepend" if is_prepend else "table",
                            example=FIELD_EXAMPLES.get(canonical, "")
                        ))
    
    return fields


def extract_paragraph_fields(root, section_name: str) -> List[FieldInfo]:
    """ë³¸ë¬¸ íŒ¨ëŸ¬ê·¸ë˜í”„ì—ì„œ í•„ë“œ ì¶”ì¶œ (ì„œëª…ë€ ë“±)"""
    fields = []
    seen_labels = set()
    
    # ì„œëª…ë€ íŒ¨í„´: "ë¼ë²¨ : ã…‡ã…‡ã…‡ (ì¸)", "ë¼ë²¨ :     (ì¸)"
    sig_pattern = re.compile(r'([ê°€-í£a-zA-Z]+)\s*[:ï¼š]\s*(ã…‡+|_+|\s+)\s*\((ì¸|å°)\)')
    
    for t_elem in root.iter(f'{HP_NS}t'):
        text = t_elem.text or ''
        
        # ì„œëª…ë€ íŒ¨í„´ ë§¤ì¹­
        match = sig_pattern.search(text)
        if match:
            label = match.group(1)
            canonical = find_canonical_label(label)
            
            if canonical not in seen_labels:
                seen_labels.add(canonical)
                fields.append(FieldInfo(
                    label=label,
                    normalized=canonical,
                    section=section_name,
                    page_hint="ì„œëª…ë€",
                    context=text[:50],
                    pattern_type="signature",
                    example=FIELD_EXAMPLES.get(canonical, "")
                ))
    
    return fields


def extract_date_fields(root, section_name: str) -> List[FieldInfo]:
    """ë‚ ì§œ í•„ë“œ ì¶”ì¶œ"""
    fields = []
    
    # ë‚ ì§œ íŒ¨í„´: "ë…„ ì›” ì¼", "2025ë…„   ì›”   ì¼"
    date_patterns = [
        (r'\d{4}ë…„\s+ì›”\s+ì¼', "YYYYë…„ ì›” ì¼ í˜•ì‹"),
        (r'\d{2}\s*ë…„\s+ì›”\s+ì¼', "YYë…„ ì›” ì¼ í˜•ì‹"),
        (r'ë…„\s+ì›”\s+ì¼', "ë…„ ì›” ì¼ í˜•ì‹"),
    ]
    
    found_date = False
    for t_elem in root.iter(f'{HP_NS}t'):
        text = t_elem.text or ''
        for pattern, desc in date_patterns:
            if re.search(pattern, text) and not found_date:
                fields.append(FieldInfo(
                    label="ë‚ ì§œ",
                    normalized="ì‹ ì²­ì¼",
                    section=section_name,
                    page_hint="ë‚ ì§œ í•„ë“œ",
                    context=desc,
                    pattern_type="date",
                    example="AUTO_DATE (ìë™) ë˜ëŠ” 2025-11-30"
                ))
                found_date = True
                break
    
    return fields


def analyze_hwpx(hwpx_path: str) -> Tuple[List[FieldInfo], Dict]:
    """HWPX íŒŒì¼ ë¶„ì„"""
    all_fields = []
    meta = {
        "filename": Path(hwpx_path).name,
        "sections": [],
        "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M")
    }
    
    with zipfile.ZipFile(hwpx_path, 'r') as zf:
        # ì„¹ì…˜ íŒŒì¼ ëª©ë¡
        section_files = sorted([f for f in zf.namelist() if f.startswith('Contents/section') and f.endswith('.xml')])
        
        for section_file in section_files:
            section_name = Path(section_file).stem  # section0, section1, ...
            meta["sections"].append(section_name)
            
            content = zf.read(section_file).decode('utf-8')
            root = ET.fromstring(content)
            
            # ê° íŒ¨í„´ë³„ í•„ë“œ ì¶”ì¶œ
            all_fields.extend(extract_table_fields(root, section_name))
            all_fields.extend(extract_paragraph_fields(root, section_name))
            all_fields.extend(extract_date_fields(root, section_name))
    
    # ì¤‘ë³µ ì œê±° (normalized ê¸°ì¤€)
    seen = set()
    unique_fields = []
    for f in all_fields:
        if f.normalized not in seen:
            seen.add(f.normalized)
            unique_fields.append(f)
    
    return unique_fields, meta


def generate_yaml_template(fields: List[FieldInfo], meta: Dict) -> str:
    """ë¹ˆ ì…ë ¥ í…œí”Œë¦¿ YAML ìƒì„±"""
    lines = [
        f"# {meta['filename']} - ë°ì´í„° ì…ë ¥ í…œí”Œë¦¿",
        f"# ìƒì„±ì¼: {meta['analyzed_at']}",
        f"# ì„¹ì…˜: {', '.join(meta['sections'])}",
        "#",
        "# ì‚¬ìš©ë²•: ê° í•„ë“œì˜ valueì— ì‹¤ì œ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”",
        "# ============================================================",
        "",
        "fields:"
    ]
    
    current_section = None
    for f in fields:
        if f.section != current_section:
            current_section = f.section
            lines.append(f"  # --- {current_section} ---")
        
        comment = f"# {f.example}" if f.example else f"# {f.context}"
        lines.append(f"  - label: \"{f.normalized}\"")
        lines.append(f"    value: \"\"                    {comment}")
        lines.append("")
    
    lines.extend([
        "keywords: {}",
        "tables: {}"
    ])
    
    return '\n'.join(lines)


def generate_prompt(fields: List[FieldInfo], meta: Dict) -> str:
    """ë°ì´í„° ìˆ˜ì§‘ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    lines = [
        f"# ğŸ“‹ {meta['filename']} ì‘ì„± ë„ìš°ë¯¸",
        "",
        f"ì´ ë¬¸ì„œ ì‘ì„±ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "",
        "ì•„ë˜ ì •ë³´ë“¤ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "ì˜ ëª¨ë¥´ê±°ë‚˜ í•´ë‹¹ ì—†ëŠ” í•­ëª©ì€ \"ì—†ìŒ\"ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.",
        "",
        "---",
        "",
        "## ì…ë ¥ í•„ìš” í•­ëª©",
        "",
        "| í•­ëª© | ì…ë ¥ ì˜ˆì‹œ | ìœ„ì¹˜ |",
        "|------|----------|------|"
    ]
    
    for f in fields:
        example = f.example if f.example else "-"
        location = f"{f.section} ({f.page_hint})"
        lines.append(f"| **{f.normalized}** | {example} | {location} |")
    
    lines.extend([
        "",
        "---",
        "",
        "## ì…ë ¥ ë°©ë²•",
        "",
        "ê° í•­ëª©ì— ëŒ€í•´ ìˆœì„œëŒ€ë¡œ ë‹µë³€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.",
        "",
        "ì˜ˆì‹œ:",
        "```",
        "ê¸°ì—…ëª…: (ì£¼)ìŠ¤ë§ˆíŠ¸í…Œí¬",
        "ëŒ€í‘œì: í™ê¸¸ë™",
        "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸: 123-45-67890",
        "...",
        "```",
        "",
        "---",
        "",
        "ìœ„ í•­ëª©ë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”!"
    ])
    
    return '\n'.join(lines)


def main():
    parser = argparse.ArgumentParser(description='HWPX í•„ë“œ ë¶„ì„ê¸°')
    parser.add_argument('hwpx', help='ë¶„ì„í•  HWPX íŒŒì¼')
    parser.add_argument('-o', '--output', help='ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬', default=None)
    parser.add_argument('--yaml-only', action='store_true', help='YAMLë§Œ ì¶œë ¥')
    parser.add_argument('--prompt-only', action='store_true', help='í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ë¶„ì„
    print(f"ğŸ“‚ ë¶„ì„ ì¤‘: {args.hwpx}")
    fields, meta = analyze_hwpx(args.hwpx)
    print(f"âœ“ {len(fields)}ê°œ í•„ë“œ ë°œê²¬")
    
    # ì¶œë ¥ íŒŒì¼ëª… ê²°ì •
    if args.output:
        prefix = args.output
    else:
        prefix = Path(args.hwpx).stem
    
    # YAML í…œí”Œë¦¿ ìƒì„±
    if not args.prompt_only:
        yaml_content = generate_yaml_template(fields, meta)
        yaml_path = f"{prefix}_fields.yaml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        print(f"âœ“ ì €ì¥: {yaml_path}")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if not args.yaml_only:
        prompt_content = generate_prompt(fields, meta)
        prompt_path = f"{prefix}_prompt.md"
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(prompt_content)
        print(f"âœ“ ì €ì¥: {prompt_path}")
    
    # ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“‹ ë°œê²¬ëœ í•„ë“œ:")
    for f in fields:
        print(f"  - {f.normalized} ({f.pattern_type}, {f.section})")


if __name__ == "__main__":
    main()
